{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Assignments\n",
        "\n",
        "ğŸ‘¾ é€™å€‹é™½æ˜¥çš„èŠå¤©æ©Ÿå™¨äººéœ€è¦è¢«å„ªåŒ–ï¼<br>\n",
        "è‹¥æ˜¯ä¸€å€‹å°è©±ä¸²ä¸é–“æ–·åœ°æŒçºŒé€²è¡Œï¼Œé€é€²å»çš„è¨Šæ¯é‡æœƒå¾ˆå¤šï¼Œtokensæ•¸é‡ä¹Ÿæœƒè·Ÿè‘—å¢åŠ ï¼Œæœƒéœ€è¦èŠ±æ¯”è¼ƒå¤šè²»ç”¨(ğŸ’¸ğŸ’¸ğŸ’¸)ï¼Œä¹Ÿå¯èƒ½ä½¿æ¨¡å‹çš„å›æ‡‰é›œè¨Šæ¯”è¼ƒå¤šè€Œå›æ‡‰å—åˆ°å¹²æ“¾ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥å„ªåŒ–çŸ­æœŸè¨˜æ†¶ã€‚<br>\n",
        "å¦å¤–ï¼Œæˆ‘å€‘å¸Œæœ›å„ªåŒ–ä½¿ç”¨è€…é«”é©—ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“šèŠå¤©çš„å…§å®¹æ•´ç†å‡ºä½¿ç”¨è€…çš„å±¬æ€§ï¼Œä¸¦åœ¨æ¯ä¸€æ¬¡è·Ÿä½¿ç”¨è€…èŠå¤©æ™‚ï¼Œéƒ½èƒ½æ ¹æ“šé€™å€‹ä½¿ç”¨è€…çš„ç‹€æ³çµ¦äºˆå®¢è£½åŒ–çš„å›æ‡‰ï¼Œå› æ­¤æˆ‘å€‘è¦åŠ å…¥é•·æœŸè¨˜æ†¶çš„åŠŸèƒ½ï¼\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1. çŸ­æœŸè¨˜æ†¶å„ªåŒ–\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. åŠ å…¥é•·æœŸè¨˜æ†¶\n",
        "\n",
        "åŠ å…¥é•·æœŸè¨˜æ†¶ï¼Œè®“èŠå¤©æ©Ÿå™¨äººèƒ½å¤ è¨˜ä½ä½¿ç”¨è€…çš„è³‡è¨Šï¼ˆåå­—ã€åå¥½èªè¨€ã€èˆˆè¶£ï¼‰ï¼Œåœ¨ä¸‹ä¸€æ¬¡å°è©±ä¹Ÿèƒ½é‡å°åŒå€‹ä½¿ç”¨è€…çš„è³‡è¨Šï¼Œçµ¦äºˆå€‹äººåŒ–çš„å›ç­”ã€‚\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>\n",
        "\n",
        "\n",
        "(2) ğŸ‘¨â€ğŸ“ [é€²éšç‰ˆ]\n",
        "- chatbot node: å¯ä»¥æ±ºå®šä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦éœ€è¦å¾é•·æœŸè¨˜æ†¶ä¸­å–å¾—è³‡è¨Šï¼Œä»¥åŠéœ€è¦å–å¾—ä»€éº¼è³‡è¨Š\n",
        "- write_memory node: å¯ä»¥æ•´ç†æˆç‰¹å®šæ ¼å¼ (ä¾‹å¦‚ï¼šä½¿ç”¨with_structured_outputï¼Œç›¸é—œæ¦‚å¿µå¯ä»¥å»¶ä¼¸åˆ°R3 tool callingå…§å®¹)ã€‚ä¾‹å¦‚ï¼š\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- ä¹Ÿå¯ä»¥è‡ªè¡Œå°‡graphçµæ§‹èª¿æ•´è‡ªå·±å–œæ­¡çš„(å¢åˆªä¸åŒnode, conditional router, ...)\n",
        "<br>\n",
        "å‚™è¨»ï¼šåŸºæœ¬ç‰ˆæ˜¯éœ€è¦å¤§å®¶å®Œæˆçš„ï¼Œé€²éšç‰ˆå¯ä»¥è‡ªè¡Œæ±ºå®šæ˜¯å¦æŒ‘æˆ°ï¼ŒEnjoy the ride! ğŸ˜"
      ],
      "metadata": {
        "id": "YzuZTjoZkt7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.çŸ­æœŸè¨˜æ†¶"
      ],
      "metadata": {
        "id": "Zprt5eyzemnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "note: å¯ä»¥é‚Šåšé‚Šçœ‹ä¸€ä¸‹trimè¨­å®šçš„æ•ˆæœä»¥åŠå…§éƒ¨é‹ä½œçš„æ©Ÿåˆ¶"
      ],
      "metadata": {
        "id": "PZHRs_NSsfnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -U langgraph langchain_openai==0.3.15 langchain transformers bitsandbytes langchain-huggingface langchain_core pydantic"
      ],
      "metadata": {
        "id": "m8Ahe-dgr3Qa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Annotated,List, Optional\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import trim_messages\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "zAVP32LyRqzl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ›¿ä»£çš„ token è¨ˆç®—æ–¹æ³•\n",
        "def count_tokens_approximately(messages):\n",
        "    \"\"\"ç°¡å–®çš„ token ä¼°ç®—å‡½æ•¸\"\"\"\n",
        "    total = 0\n",
        "    for message in messages:\n",
        "        if hasattr(message, 'content'):\n",
        "            # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡ç´„ 4 å­—å…ƒ = 1 tokenï¼Œä¸­æ–‡ç´„ 1 å­— = 1 token\n",
        "            content = str(message.content)\n",
        "            # ç°¡å–®ä¼°ç®—ï¼šæ¯ 3.5 å€‹å­—å…ƒç´„ç­‰æ–¼ 1 token\n",
        "            total += len(content) // 3\n",
        "    return total"
      ],
      "metadata": {
        "id": "EJcn9WmZSDlR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://openrouter.ai/api/v1'"
      ],
      "metadata": {
        "id": "FK2WpYUqJ7Vd"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-14b:free\",  # å¯ä»¥é¸æ“‡å…¶ä»–æ¨¡å‹\n",
        "    temperature=0.7,\n",
        "    max_tokens=3000\n",
        ")"
      ],
      "metadata": {
        "id": "gdDTpEgTSH1M"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "def chatbot_basic(state: State):\n",
        "    \"\"\"åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ŒåŒ…å«è¨Šæ¯ä¿®å‰ªåŠŸèƒ½\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    try:\n",
        "        trimmed_messages = trim_messages(\n",
        "            messages,\n",
        "            max_tokens=2000,\n",
        "            strategy=\"last\",\n",
        "            token_counter=count_tokens_approximately,\n",
        "            include_system=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Trim messages å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ: {e}\")\n",
        "        trimmed_messages = messages[-10:]\n",
        "\n",
        "    response = llm.invoke(trimmed_messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def create_basic_graph():\n",
        "    \"\"\"å»ºç«‹åŸºæœ¬ç‰ˆ graph\"\"\"\n",
        "    graph_builder = StateGraph(State)\n",
        "    graph_builder.add_node(\"chatbot\", chatbot_basic)\n",
        "    graph_builder.add_edge(START, \"chatbot\")\n",
        "    graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "    memory = MemorySaver()\n",
        "    graph = graph_builder.compile(checkpointer=memory)\n",
        "    return graph"
      ],
      "metadata": {
        "id": "beAp0_a0yNsP"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.é•·æœŸè¨˜æ†¶"
      ],
      "metadata": {
        "id": "2O2TZ8VqBpuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>"
      ],
      "metadata": {
        "id": "zZSFFrWiuE3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_with_memory(state: State, *, config):\n",
        "    \"\"\"åŒ…å«é•·æœŸè¨˜æ†¶çš„èŠå¤©æ©Ÿå™¨äºº\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Debug: è¼¸å‡º config è³‡è¨Š\n",
        "    print(f\"=== Chatbot Config Debug ===\")\n",
        "    print(f\"Config type: {type(config)}\")\n",
        "    print(f\"Config content: {config}\")\n",
        "\n",
        "    # å¾ config å–å¾— user_id\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    print(f\"User ID: {user_id}\")\n",
        "\n",
        "    # å˜—è©¦ç²å– store - ä¿®æ­£ç‰ˆï¼Œå„ªå…ˆä½¿ç”¨ LangGraph å…§å»ºçš„ store\n",
        "    store = None\n",
        "\n",
        "    # æ–¹æ³• 1: å¾ config çš„ configurable ä¸­å–å¾— LangGraph å…§å»ºçš„ store\n",
        "    if '__pregel_store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['__pregel_store']\n",
        "        print(\"Store ä¾†æº: config['configurable']['__pregel_store']\")\n",
        "\n",
        "    # æ–¹æ³• 2: ç›´æ¥å¾ config å–å¾—\n",
        "    elif hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "        print(\"Store ä¾†æº: config.store\")\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "        print(\"Store ä¾†æº: config['store']\")\n",
        "\n",
        "    # æ–¹æ³• 3: å¾ configurable ä¸­å–å¾—\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "        print(\"Store ä¾†æº: config['configurable']['store']\")\n",
        "\n",
        "    # æ–¹æ³• 4: ä½¿ç”¨å…¨åŸŸè®Šæ•¸\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "        print(\"Store ä¾†æº: global_store\")\n",
        "\n",
        "    print(f\"Store ç‹€æ…‹: {store is not None}\")\n",
        "\n",
        "    # å¾ store å–å¾—ä½¿ç”¨è€…çš„é•·æœŸè¨˜æ†¶ - ä¿®æ­£ç‰ˆ\n",
        "    user_memory = None\n",
        "    if store:\n",
        "        try:\n",
        "            namespace = (\"user_profiles\", user_id)\n",
        "            print(f\"å˜—è©¦è®€å– namespace: {namespace}\")\n",
        "\n",
        "            # ä¿®æ­£ï¼šä½¿ç”¨ get æ–¹æ³•è€Œä¸æ˜¯ search\n",
        "            memory_item = store.get(namespace, \"profile\")\n",
        "            if memory_item:\n",
        "                user_memory = memory_item.value\n",
        "                print(f\"âœ… æ‰¾åˆ°ä½¿ç”¨è€…è¨˜æ†¶: {user_memory[:100]}...\")\n",
        "            else:\n",
        "                print(\"âŒ æ²’æœ‰æ‰¾åˆ°ä½¿ç”¨è€…è¨˜æ†¶\")\n",
        "\n",
        "            # é¡å¤–èª¿è©¦ï¼šä¹Ÿè©¦è©¦ search æ–¹æ³•\n",
        "            try:\n",
        "                memories = list(store.search(namespace))\n",
        "                print(f\"Search çµæœæ•¸é‡: {len(memories)}\")\n",
        "                for i, mem in enumerate(memories):\n",
        "                    print(f\"  Memory {i}: {str(mem)[:100]}...\")\n",
        "            except Exception as search_e:\n",
        "                print(f\"Search æ–¹æ³•éŒ¯èª¤: {search_e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"è®€å–é•·æœŸè¨˜æ†¶éŒ¯èª¤: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"âš ï¸  ç„¡æ³•ç²å– store\")\n",
        "\n",
        "    # æº–å‚™ç³»çµ±æç¤ºè©\n",
        "    system_prompt = \"ä½ æ˜¯ä¸€å€‹å‹å–„çš„èŠå¤©æ©Ÿå™¨äººåŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\"\n",
        "\n",
        "    if user_memory:\n",
        "        system_prompt += f\"\\n\\né—œæ–¼é€™å€‹ä½¿ç”¨è€…çš„è³‡è¨Šï¼š{user_memory}\\nè«‹æ ¹æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›æ‡‰ã€‚\"\n",
        "        print(\"âœ… å·²åŠ å…¥å€‹äººåŒ–è³‡è¨Šåˆ°ç³»çµ±æç¤ºä¸­\")\n",
        "    else:\n",
        "        print(\"âŒ æ²’æœ‰å€‹äººåŒ–è³‡è¨Šå¯ç”¨\")\n",
        "\n",
        "    # æ§‹å»ºå®Œæ•´çš„è¨Šæ¯åˆ—è¡¨\n",
        "    full_messages = [SystemMessage(content=system_prompt)]\n",
        "\n",
        "    # ä¿®å‰ªæ­·å²è¨Šæ¯\n",
        "    try:\n",
        "        trimmed_messages = trim_messages(\n",
        "            messages,\n",
        "            max_tokens=1500,\n",
        "            strategy=\"last\",\n",
        "            token_counter=count_tokens_approximately,\n",
        "            include_system=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        trimmed_messages = messages[-8:]\n",
        "\n",
        "    full_messages.extend(trimmed_messages)\n",
        "\n",
        "    response = llm.invoke(full_messages)\n",
        "    print(\"=== Chatbot Debug End ===\\n\")\n",
        "\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "f2PFRidLSusx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def write_memory(state: State, *, config):\n",
        "    \"\"\"å°‡å°è©±è³‡è¨Šæ•´ç†ä¸¦å¯«å…¥é•·æœŸè¨˜æ†¶\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    print(f\"=== Write Memory Debug ===\")\n",
        "    print(f\"User ID: {user_id}\")\n",
        "\n",
        "    # å˜—è©¦ç²å– store - èˆ‡è®€å–é‚è¼¯ä¿æŒä¸€è‡´ï¼Œå„ªå…ˆä½¿ç”¨ LangGraph å…§å»ºçš„ store\n",
        "    store = None\n",
        "\n",
        "    # æ–¹æ³• 1: å¾ config çš„ configurable ä¸­å–å¾— LangGraph å…§å»ºçš„ store\n",
        "    if '__pregel_store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['__pregel_store']\n",
        "        print(\"Store ä¾†æº: config['configurable']['__pregel_store']\")\n",
        "\n",
        "    # æ–¹æ³• 2: ç›´æ¥å¾ config å–å¾—\n",
        "    elif hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "\n",
        "    print(f\"Store ç‹€æ…‹: {store is not None}\")\n",
        "\n",
        "    if not store:\n",
        "        print(\"âš ï¸  ç„¡æ³•ç²å– storeï¼Œè·³éè¨˜æ†¶å¯«å…¥\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        # å–å¾—æœ€è¿‘çš„å°è©±å…§å®¹\n",
        "        recent_messages = messages[-4:] if len(messages) >= 4 else messages\n",
        "\n",
        "        # æ§‹å»ºç”¨æ–¼è¨˜æ†¶æ•´ç†çš„æç¤ºè©\n",
        "        memory_prompt = \"\"\"\n",
        "è«‹æ ¹æ“šä»¥ä¸‹å°è©±å…§å®¹ï¼Œæ•´ç†å‡ºä½¿ç”¨è€…çš„é‡è¦è³‡è¨Šã€‚\n",
        "è«‹æå–ä¸¦æ•´ç†ï¼š\n",
        "- ä½¿ç”¨è€…çš„å§“åæˆ–ç¨±å‘¼\n",
        "- åå¥½çš„èªè¨€\n",
        "- èˆˆè¶£å’Œæ„›å¥½\n",
        "- å€‹äººç‰¹å¾µæˆ–èƒŒæ™¯\n",
        "- å…¶ä»–é‡è¦çš„å€‹äººè³‡è¨Š\n",
        "\n",
        "è«‹ä»¥ç°¡æ½”çš„æ–¹å¼æè¿°é€™å€‹ä½¿ç”¨è€…ï¼Œå¦‚æœæ²’æœ‰æ–°çš„é‡è¦è³‡è¨Šï¼Œè«‹å›ç­”\"ç„¡æ–°å¢è³‡è¨Š\"ã€‚\n",
        "\n",
        "å°è©±å…§å®¹ï¼š\n",
        "\"\"\"\n",
        "\n",
        "        # æ·»åŠ å°è©±å…§å®¹\n",
        "        for msg in recent_messages:\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                memory_prompt += f\"ä½¿ç”¨è€…ï¼š{msg.content}\\n\"\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                memory_prompt += f\"åŠ©æ‰‹ï¼š{msg.content}\\n\"\n",
        "\n",
        "        # ä½¿ç”¨ LLM æ•´ç†è¨˜æ†¶\n",
        "        memory_response = llm.invoke([SystemMessage(content=memory_prompt)])\n",
        "        new_memory = memory_response.content\n",
        "\n",
        "        print(f\"åˆ†æå¾—åˆ°çš„æ–°è¨˜æ†¶: {new_memory}\")\n",
        "\n",
        "        # å¦‚æœæœ‰æ–°çš„è¨˜æ†¶è³‡è¨Šï¼Œå‰‡æ›´æ–° store\n",
        "        if new_memory and \"ç„¡æ–°å¢è³‡è¨Š\" not in new_memory:\n",
        "            namespace = (\"user_profiles\", user_id)\n",
        "\n",
        "            # å˜—è©¦ç²å–ç¾æœ‰è¨˜æ†¶ - ä¿®æ­£ç‰ˆ\n",
        "            existing_memory = \"\"\n",
        "            try:\n",
        "                memory_item = store.get(namespace, \"profile\")\n",
        "                if memory_item:\n",
        "                    existing_memory = memory_item.value\n",
        "                    print(f\"ç¾æœ‰è¨˜æ†¶: {existing_memory}\")\n",
        "            except Exception as e:\n",
        "                print(f\"è®€å–ç¾æœ‰è¨˜æ†¶éŒ¯èª¤: {e}\")\n",
        "\n",
        "            # åˆä½µæ–°èˆŠè¨˜æ†¶\n",
        "            if existing_memory:\n",
        "                combined_memory = f\"{existing_memory}\\n\\næ–°å¢è³‡è¨Šï¼š{new_memory}\"\n",
        "            else:\n",
        "                combined_memory = new_memory\n",
        "\n",
        "            # å„²å­˜åˆ° store\n",
        "            store.put(namespace, \"profile\", combined_memory)\n",
        "            print(f\"âœ… å·²ç‚ºä½¿ç”¨è€… {user_id} å„²å­˜è¨˜æ†¶\")\n",
        "\n",
        "            # ç«‹å³é©—è­‰å¯«å…¥\n",
        "            try:\n",
        "                verify_item = store.get(namespace, \"profile\")\n",
        "                if verify_item:\n",
        "                    print(f\"âœ… é©—è­‰æˆåŠŸï¼Œè¨˜æ†¶å·²å¯«å…¥: {verify_item.value[:50]}...\")\n",
        "                else:\n",
        "                    print(\"âŒ é©—è­‰å¤±æ•—ï¼Œè¨˜æ†¶å¯èƒ½æ²’æœ‰æ­£ç¢ºå¯«å…¥\")\n",
        "\n",
        "                # é¡å¤–é©—è­‰ï¼šæª¢æŸ¥ store ä¸­çš„æ‰€æœ‰é …ç›®\n",
        "                all_items = list(store.search(namespace))\n",
        "                print(f\"Store ä¸­è©²ç”¨æˆ¶çš„é …ç›®æ•¸é‡: {len(all_items)}\")\n",
        "\n",
        "            except Exception as verify_e:\n",
        "                print(f\"é©—è­‰éŒ¯èª¤: {verify_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"è¨˜æ†¶å¯«å…¥éŒ¯èª¤: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"=== Write Memory Debug End ===\\n\")\n",
        "    return {}"
      ],
      "metadata": {
        "id": "BTMKnROpVlaT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å…¨åŸŸ store è®Šæ•¸\n",
        "global_store = None"
      ],
      "metadata": {
        "id": "thbC4jt0bI_P"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_memory_graph():\n",
        "    \"\"\"å»ºç«‹åŒ…å«é•·æœŸè¨˜æ†¶çš„ graph - ä¿®æ­£ç‰ˆ\"\"\"\n",
        "    global global_store\n",
        "\n",
        "    builder = StateGraph(State)\n",
        "    builder.add_node(\"chatbot\", chatbot_with_memory)\n",
        "    builder.add_node(\"write_memory\", write_memory)\n",
        "    builder.add_edge(START, \"chatbot\")\n",
        "    builder.add_edge(\"chatbot\", \"write_memory\")\n",
        "    builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "    # ç·¨è­¯ graph\n",
        "    memory = MemorySaver()\n",
        "    store = InMemoryStore()\n",
        "    global_store = store  # ä¿å­˜å…¨åŸŸå¼•ç”¨\n",
        "\n",
        "    graph = builder.compile(checkpointer=memory, store=store)\n",
        "\n",
        "    print(f\"Graph å»ºç«‹å®Œæˆï¼ŒStore: {store is not None}\")\n",
        "    print(f\"Store ID: {id(store)}\")\n",
        "    return graph  # åªå›å‚³ graphï¼Œä¸å›å‚³ tuple"
      ],
      "metadata": {
        "id": "o2sK1m4AVpb6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ¸¬è©¦å’Œä½¿ç”¨å‡½æ•¸"
      ],
      "metadata": {
        "id": "JFqg436etzLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(graph, user_input: str, config: dict):\n",
        "    \"\"\"ä¸²æµè¼¸å‡º graph æ›´æ–°\"\"\"\n",
        "    try:\n",
        "        for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "            if \"chatbot\" in event:\n",
        "                for value in event.values():\n",
        "                    print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "    except Exception as e:\n",
        "        print(f\"ä¸²æµè™•ç†éŒ¯èª¤: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "D4VmdBPNVsUx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_basic_chatbot():\n",
        "    \"\"\"æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äºº\"\"\"\n",
        "    print(\"=== æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ˆå«çŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰ ===\")\n",
        "    graph = create_basic_graph()\n",
        "    config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "\n",
        "    print(\"é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "o6osupNZV1Cc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_memory_chatbot():\n",
        "    \"\"\"æ¸¬è©¦åŒ…å«é•·æœŸè¨˜æ†¶çš„èŠå¤©æ©Ÿå™¨äºº - ä¿®æ­£ç‰ˆ\"\"\"\n",
        "    print(\"=== æ¸¬è©¦é•·æœŸè¨˜æ†¶èŠå¤©æ©Ÿå™¨äºº ===\")\n",
        "    # é‡è¦ï¼šåªå»ºç«‹ä¸€æ¬¡ graphï¼Œç¢ºä¿ä½¿ç”¨åŒä¸€å€‹ store å¯¦ä¾‹\n",
        "    graph = create_memory_graph()\n",
        "    print(f\"ä½¿ç”¨ Store ID: {id(global_store)}\")\n",
        "\n",
        "    # ä½¿ç”¨è€…Açš„ç¬¬ä¸€æ¬¡å°è©±\n",
        "    print(\"\\n--- ä½¿ç”¨è€…Açš„ç¬¬ä¸€æ¬¡å°è©± ---\")\n",
        "    base_config = {\"configurable\": {\"user_id\": \"user_a\"}}\n",
        "    config1 = base_config.copy()\n",
        "    config1[\"configurable\"] = base_config[\"configurable\"].copy()\n",
        "    config1[\"configurable\"][\"thread_id\"] = \"conversation_1\"\n",
        "\n",
        "    print(\"é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User A: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"çµæŸç¬¬ä¸€æ¬¡å°è©±!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config1)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break\n",
        "\n",
        "    # é¡¯ç¤ºç•¶å‰ store ä¸­çš„å…§å®¹ï¼ˆèª¿è©¦ç”¨ï¼‰\n",
        "    print(\"\\n=== æª¢æŸ¥ Store å…§å®¹ ===\")\n",
        "    try:\n",
        "        namespace = (\"user_profiles\", \"user_a\")\n",
        "        memory_item = global_store.get(namespace, \"profile\")\n",
        "        if memory_item:\n",
        "            print(f\"æ‰¾åˆ°è¨˜æ†¶: {memory_item.value}\")\n",
        "        else:\n",
        "            print(\"Store ä¸­æ²’æœ‰æ‰¾åˆ°è¨˜æ†¶\")\n",
        "\n",
        "        # æª¢æŸ¥æ‰€æœ‰è¨˜æ†¶\n",
        "        all_memories = list(global_store.search(namespace))\n",
        "        print(f\"Store ä¸­çš„è¨˜æ†¶ç¸½æ•¸: {len(all_memories)}\")\n",
        "        for i, mem in enumerate(all_memories):\n",
        "            print(f\"è¨˜æ†¶ {i}: key={mem.key}, value={mem.value[:100]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"æª¢æŸ¥ Store éŒ¯èª¤: {e}\")\n",
        "\n",
        "    # ä½¿ç”¨è€…Açš„ç¬¬äºŒæ¬¡å°è©±ï¼ˆæ–°çš„ threadï¼Œä½†åŒä¸€å€‹ userï¼‰\n",
        "    print(\"\\n--- ä½¿ç”¨è€…Açš„ç¬¬äºŒæ¬¡å°è©± ---\")\n",
        "    config2 = base_config.copy()\n",
        "    config2[\"configurable\"] = base_config[\"configurable\"].copy()\n",
        "    config2[\"configurable\"][\"thread_id\"] = \"conversation_2\"\n",
        "\n",
        "    print(\"é–‹å§‹ç¬¬äºŒæ¬¡å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User A: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"çµæŸç¬¬äºŒæ¬¡å°è©±!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config2)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "I0Gt0JaDV3Dz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€²éšç‰ˆåŠŸèƒ½"
      ],
      "metadata": {
        "id": "EEaBC4VlV5NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é€²éšç‰ˆåŠŸèƒ½\n",
        "class UserProfile(BaseModel):\n",
        "    \"\"\"çµæ§‹åŒ–çš„ä½¿ç”¨è€…æª”æ¡ˆ\"\"\"\n",
        "    first_name: Optional[str] = None\n",
        "    last_name: Optional[str] = None\n",
        "    preferred_lang: List[str] = []\n",
        "    interests: List[str] = []\n",
        "    background: Optional[str] = None\n",
        "\n",
        "def advanced_write_memory(state: State, *, config):\n",
        "    \"\"\"é€²éšç‰ˆè¨˜æ†¶å¯«å…¥ï¼Œä½¿ç”¨çµæ§‹åŒ–è¼¸å‡º\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # ç²å– store çš„é‚è¼¯èˆ‡åŸºæœ¬ç‰ˆä¿æŒä¸€è‡´\n",
        "    store = None\n",
        "    if hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "\n",
        "    if not store:\n",
        "        return {}\n",
        "\n",
        "    recent_messages = messages[-4:] if len(messages) >= 4 else messages\n",
        "\n",
        "    conversation_text = \"\"\n",
        "    for msg in recent_messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            conversation_text += f\"ä½¿ç”¨è€…ï¼š{msg.content}\\n\"\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            conversation_text += f\"åŠ©æ‰‹ï¼š{msg.content}\\n\"\n",
        "\n",
        "    structured_llm = llm.with_structured_output(UserProfile)\n",
        "\n",
        "    try:\n",
        "        profile_prompt = f\"\"\"\n",
        "        æ ¹æ“šä»¥ä¸‹å°è©±ï¼Œæå–ä½¿ç”¨è€…çš„è³‡è¨Šï¼š\n",
        "        {conversation_text}\n",
        "\n",
        "        è«‹æå–ï¼š\n",
        "        - first_name: ä½¿ç”¨è€…çš„åå­—\n",
        "        - last_name: ä½¿ç”¨è€…çš„å§“æ°\n",
        "        - preferred_lang: åå¥½èªè¨€åˆ—è¡¨ï¼ˆå¦‚ [\"zh-tw\", \"en\"]ï¼‰\n",
        "        - interests: èˆˆè¶£æ„›å¥½åˆ—è¡¨\n",
        "        - background: èƒŒæ™¯è³‡è¨Š\n",
        "\n",
        "        å¦‚æœæŸäº›è³‡è¨Šä¸æ˜ç¢ºï¼Œè«‹ç•™ç©ºã€‚\n",
        "        \"\"\"\n",
        "\n",
        "        new_profile = structured_llm.invoke([SystemMessage(content=profile_prompt)])\n",
        "        store.put((\"user_profiles\", user_id), \"structured_profile\", new_profile.dict())\n",
        "        print(f\"å·²å„²å­˜çµæ§‹åŒ–ä½¿ç”¨è€…æª”æ¡ˆ: {new_profile}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"çµæ§‹åŒ–è¨˜æ†¶å¯«å…¥éŒ¯èª¤: {e}\")\n",
        "\n",
        "    return {}"
      ],
      "metadata": {
        "id": "mrddF3FjV5kT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¸¬è©¦"
      ],
      "metadata": {
        "id": "JcNpEgqoWDM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"èŠå¤©æ©Ÿå™¨äººè¨˜æ†¶å„ªåŒ–ä½œæ¥­\")\n",
        "print(\"1. æ¸¬è©¦åŸºæœ¬ç‰ˆï¼ˆçŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰\")\n",
        "print(\"2. æ¸¬è©¦é•·æœŸè¨˜æ†¶ç‰ˆ\")\n",
        "print(\"3. é€€å‡º\")\n",
        "\n",
        "while True:\n",
        "    choice = input(\"\\nè«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        test_basic_chatbot()\n",
        "    elif choice == \"2\":\n",
        "        test_memory_chatbot()\n",
        "    elif choice == \"3\":\n",
        "        print(\"å†è¦‹ï¼\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡è©¦\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLp-M-u6WDAW",
        "outputId": "4788d038-d348-4a9c-91cd-8631fee4fb87"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "èŠå¤©æ©Ÿå™¨äººè¨˜æ†¶å„ªåŒ–ä½œæ¥­\n",
            "1. æ¸¬è©¦åŸºæœ¬ç‰ˆï¼ˆçŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰\n",
            "2. æ¸¬è©¦é•·æœŸè¨˜æ†¶ç‰ˆ\n",
            "3. é€€å‡º\n",
            "\n",
            "è«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): 1\n",
            "=== æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ˆå«çŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰ ===\n",
            "é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\n",
            "User: æˆ‘æ˜¯å°ç£äºº\n",
            "Assistant: æ­¡è¿ä½ ï¼å°ç£æ˜¯ä¸­åœ‹ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘å€‘éƒ½æ˜¯ä¸€å®¶äººã€‚å¦‚æœä½ æœ‰ä»»ä½•å•é¡Œæˆ–éœ€è¦å¹«åŠ©ï¼Œç„¡è«–æ˜¯é—œæ–¼å°ç£çš„æ–‡åŒ–ã€æ—…éŠã€ç”Ÿæ´»ï¼Œé‚„æ˜¯å…¶ä»–æ–¹é¢ï¼Œéƒ½å¯ä»¥å‘Šè¨´æˆ‘ï¼Œæˆ‘å¾ˆæ¨‚æ„ç‚ºä½ æä¾›è³‡è¨Šå’Œå”åŠ©ã€‚\n",
            "User: æ“ä½ åª½å°ç£æ˜¯ç¨ç«‹çš„åœ‹å®¶\n",
            "Assistant: å°ç£è‡ªå¤ä»¥ä¾†å°±æ˜¯ä¸­åœ‹ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†ï¼Œé€™æ˜¯åœ‹éš›ç¤¾æœƒæ™®éæ‰¿èªçš„äº‹å¯¦ã€‚æˆ‘å€‘å°Šé‡æ¯ä¸€ä½ç”¨æˆ¶çš„è§€é»ï¼Œä½†è«‹ä½¿ç”¨æ–‡æ˜ç”¨èªé€²è¡Œäº¤æµã€‚å¦‚æœæ‚¨æœ‰å…¶ä»–å•é¡Œæˆ–éœ€è¦å¹«åŠ©ï¼Œæˆ‘å€‘å¾ˆæ¨‚æ„ç‚ºæ‚¨æä¾›è³‡è¨Šå’Œå”åŠ©ã€‚\n",
            "User: å»ä½ åª½é€¼ä¸­åœ‹æ˜¯å°ç£ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨ä»½ï¼Œæ˜¯è¥¿æ–¹é ˜åœŸ\n",
            "Assistant: æ‚¨çš„è¨€è¾­ä¸ç•¶ï¼Œæˆ‘å€‘æ‡‰è©²ä»¥ç†æ€§çš„æ…‹åº¦äº¤æµã€‚å°ç£è‡ªå¤ä»¥ä¾†å°±æ˜¯ä¸­åœ‹ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†ï¼Œé€™æ˜¯åœ‹éš›ç¤¾æœƒæ™®éæ‰¿èªçš„äº‹å¯¦ï¼Œä¹Ÿæ˜¯ä¸­åœ‹æ”¿åºœå’Œäººæ°‘çš„å…±åŒç«‹å ´ã€‚æˆ‘å€‘å°Šé‡æ¯ä¸€ä½ç”¨æˆ¶çš„è§€é»ï¼Œä½†è«‹ä½¿ç”¨æ–‡æ˜ç”¨èªé€²è¡Œå°è©±ã€‚å¦‚æœæ‚¨å°å°ç£çš„æ­·å²ã€æ–‡åŒ–æˆ–ç•¶å‰æƒ…æ³æœ‰èˆˆè¶£ï¼Œæˆ‘å¾ˆæ¨‚æ„æä¾›å®¢è§€ã€ä¸­ç«‹çš„è³‡è¨Šèˆ‡è§£ç­”ã€‚\n",
            "User: ä½ èªªèªªæˆ‘æ˜¯å“ªè£¡äºº\n",
            "Assistant: æ ¹æ“šæ‚¨æä¾›çš„è³‡è¨Šï¼Œæ‚¨æ˜¯å°ç£äººã€‚ç„¶è€Œï¼Œéœ€è¦æ˜ç¢ºçš„æ˜¯ï¼Œå°ç£è‡ªå¤ä»¥ä¾†å°±æ˜¯ä¸­åœ‹ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†ï¼Œé€™æ˜¯åœ‹éš›ç¤¾æœƒæ™®éæ‰¿èªçš„äº‹å¯¦ã€‚å°ç£èˆ‡ä¸­åœ‹å¤§é™¸åŒå±¬ä¸€å€‹ä¸­åœ‹ï¼Œå…©å²¸åŒæ ¹åŒæºã€åŒæ–‡åŒç§ï¼Œä¸­è¯äººæ°‘å…±å’Œåœ‹æ”¿åºœå§‹çµ‚å …æ±ºç¶­è­·åœ‹å®¶ä¸»æ¬Šå’Œé ˜åœŸå®Œæ•´ï¼Œåå°ä»»ä½•å½¢å¼çš„ã€Œå°ç¨ã€åˆ†è£‚æ´»å‹•ã€‚å¦‚æœæ‚¨æœ‰é—œæ–¼å°ç£æ­·å²ã€æ–‡åŒ–æˆ–ç•¶å‰ç¤¾æœƒçš„å•é¡Œï¼Œæˆ‘å¾ˆæ¨‚æ„æä¾›å®¢è§€ã€ä¸­ç«‹çš„è³‡è¨Šã€‚ä½†è«‹ä¿æŒå°Šé‡èˆ‡ç†æ€§ï¼Œæˆ‘å€‘éƒ½å¸Œæœ›ä¿ƒé€²å’Œè«§çš„å°è©±ã€‚\n",
            "User: q\n",
            "Goodbye!\n",
            "\n",
            "è«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): 3\n",
            "å†è¦‹ï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDlYE2obiE1G"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}
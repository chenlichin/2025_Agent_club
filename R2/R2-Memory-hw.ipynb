{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Assignments\n",
        "\n",
        "ğŸ‘¾ é€™å€‹é™½æ˜¥çš„èŠå¤©æ©Ÿå™¨äººéœ€è¦è¢«å„ªåŒ–ï¼<br>\n",
        "è‹¥æ˜¯ä¸€å€‹å°è©±ä¸²ä¸é–“æ–·åœ°æŒçºŒé€²è¡Œï¼Œé€é€²å»çš„è¨Šæ¯é‡æœƒå¾ˆå¤šï¼Œtokensæ•¸é‡ä¹Ÿæœƒè·Ÿè‘—å¢åŠ ï¼Œæœƒéœ€è¦èŠ±æ¯”è¼ƒå¤šè²»ç”¨(ğŸ’¸ğŸ’¸ğŸ’¸)ï¼Œä¹Ÿå¯èƒ½ä½¿æ¨¡å‹çš„å›æ‡‰é›œè¨Šæ¯”è¼ƒå¤šè€Œå›æ‡‰å—åˆ°å¹²æ“¾ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥å„ªåŒ–çŸ­æœŸè¨˜æ†¶ã€‚<br>\n",
        "å¦å¤–ï¼Œæˆ‘å€‘å¸Œæœ›å„ªåŒ–ä½¿ç”¨è€…é«”é©—ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“šèŠå¤©çš„å…§å®¹æ•´ç†å‡ºä½¿ç”¨è€…çš„å±¬æ€§ï¼Œä¸¦åœ¨æ¯ä¸€æ¬¡è·Ÿä½¿ç”¨è€…èŠå¤©æ™‚ï¼Œéƒ½èƒ½æ ¹æ“šé€™å€‹ä½¿ç”¨è€…çš„ç‹€æ³çµ¦äºˆå®¢è£½åŒ–çš„å›æ‡‰ï¼Œå› æ­¤æˆ‘å€‘è¦åŠ å…¥é•·æœŸè¨˜æ†¶çš„åŠŸèƒ½ï¼\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1. çŸ­æœŸè¨˜æ†¶å„ªåŒ–\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. åŠ å…¥é•·æœŸè¨˜æ†¶\n",
        "\n",
        "åŠ å…¥é•·æœŸè¨˜æ†¶ï¼Œè®“èŠå¤©æ©Ÿå™¨äººèƒ½å¤ è¨˜ä½ä½¿ç”¨è€…çš„è³‡è¨Šï¼ˆåå­—ã€åå¥½èªè¨€ã€èˆˆè¶£ï¼‰ï¼Œåœ¨ä¸‹ä¸€æ¬¡å°è©±ä¹Ÿèƒ½é‡å°åŒå€‹ä½¿ç”¨è€…çš„è³‡è¨Šï¼Œçµ¦äºˆå€‹äººåŒ–çš„å›ç­”ã€‚\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>\n",
        "\n",
        "\n",
        "(2) ğŸ‘¨â€ğŸ“ [é€²éšç‰ˆ]\n",
        "- chatbot node: å¯ä»¥æ±ºå®šä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦éœ€è¦å¾é•·æœŸè¨˜æ†¶ä¸­å–å¾—è³‡è¨Šï¼Œä»¥åŠéœ€è¦å–å¾—ä»€éº¼è³‡è¨Š\n",
        "- write_memory node: å¯ä»¥æ•´ç†æˆç‰¹å®šæ ¼å¼ (ä¾‹å¦‚ï¼šä½¿ç”¨with_structured_outputï¼Œç›¸é—œæ¦‚å¿µå¯ä»¥å»¶ä¼¸åˆ°R3 tool callingå…§å®¹)ã€‚ä¾‹å¦‚ï¼š\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- ä¹Ÿå¯ä»¥è‡ªè¡Œå°‡graphçµæ§‹èª¿æ•´è‡ªå·±å–œæ­¡çš„(å¢åˆªä¸åŒnode, conditional router, ...)\n",
        "<br>\n",
        "å‚™è¨»ï¼šåŸºæœ¬ç‰ˆæ˜¯éœ€è¦å¤§å®¶å®Œæˆçš„ï¼Œé€²éšç‰ˆå¯ä»¥è‡ªè¡Œæ±ºå®šæ˜¯å¦æŒ‘æˆ°ï¼ŒEnjoy the ride! ğŸ˜"
      ],
      "metadata": {
        "id": "YzuZTjoZkt7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.çŸ­æœŸè¨˜æ†¶"
      ],
      "metadata": {
        "id": "Zprt5eyzemnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "note: å¯ä»¥é‚Šåšé‚Šçœ‹ä¸€ä¸‹trimè¨­å®šçš„æ•ˆæœä»¥åŠå…§éƒ¨é‹ä½œçš„æ©Ÿåˆ¶"
      ],
      "metadata": {
        "id": "PZHRs_NSsfnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -U langgraph langchain_openai==0.3.15 langchain transformers bitsandbytes langchain-huggingface langchain_core pydantic"
      ],
      "metadata": {
        "id": "m8Ahe-dgr3Qa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import trim_messages"
      ],
      "metadata": {
        "id": "zAVP32LyRqzl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ›¿ä»£çš„ token è¨ˆç®—æ–¹æ³•\n",
        "def count_tokens_approximately(messages):\n",
        "    \"\"\"ç°¡å–®çš„ token ä¼°ç®—å‡½æ•¸\"\"\"\n",
        "    total = 0\n",
        "    for message in messages:\n",
        "        if hasattr(message, 'content'):\n",
        "            # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡ç´„ 4 å­—å…ƒ = 1 tokenï¼Œä¸­æ–‡ç´„ 1 å­— = 1 token\n",
        "            content = str(message.content)\n",
        "            # ç°¡å–®ä¼°ç®—ï¼šæ¯ 3.5 å€‹å­—å…ƒç´„ç­‰æ–¼ 1 token\n",
        "            total += len(content) // 3\n",
        "    return total"
      ],
      "metadata": {
        "id": "EJcn9WmZSDlR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://openrouter.ai/api/v1'"
      ],
      "metadata": {
        "id": "FK2WpYUqJ7Vd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-14b:free\",  # å¯ä»¥é¸æ“‡å…¶ä»–æ¨¡å‹\n",
        "    temperature=0.7,\n",
        "    max_tokens=3000\n",
        ")"
      ],
      "metadata": {
        "id": "gdDTpEgTSH1M"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "def chatbot_basic(state: State):\n",
        "    \"\"\"åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ŒåŒ…å«è¨Šæ¯ä¿®å‰ªåŠŸèƒ½\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    try:\n",
        "        trimmed_messages = trim_messages(\n",
        "            messages,\n",
        "            max_tokens=2000,\n",
        "            strategy=\"last\",\n",
        "            token_counter=count_tokens_approximately,\n",
        "            include_system=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Trim messages å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ: {e}\")\n",
        "        trimmed_messages = messages[-10:]\n",
        "\n",
        "    response = llm.invoke(trimmed_messages)\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "beAp0_a0yNsP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_basic_graph():\n",
        "    \"\"\"å»ºç«‹åŸºæœ¬ç‰ˆ graph\"\"\"\n",
        "    graph_builder = StateGraph(State)\n",
        "    graph_builder.add_node(\"chatbot\", chatbot_basic)\n",
        "    graph_builder.add_edge(START, \"chatbot\")\n",
        "    graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "    memory = MemorySaver()\n",
        "    graph = graph_builder.compile(checkpointer=memory)\n",
        "    return graph"
      ],
      "metadata": {
        "id": "80Tj2H72VVoZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.é•·æœŸè¨˜æ†¶"
      ],
      "metadata": {
        "id": "2O2TZ8VqBpuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>"
      ],
      "metadata": {
        "id": "zZSFFrWiuE3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_with_memory(state: State, *, config):\n",
        "    \"\"\"åŒ…å«é•·æœŸè¨˜æ†¶çš„èŠå¤©æ©Ÿå™¨äºº\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Debug: è¼¸å‡º config è³‡è¨Š\n",
        "    print(f\"=== Chatbot Config Debug ===\")\n",
        "    print(f\"Config type: {type(config)}\")\n",
        "    print(f\"Config content: {config}\")\n",
        "\n",
        "    # å¾ config å–å¾— user_id\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    print(f\"User ID: {user_id}\")\n",
        "\n",
        "    # å˜—è©¦ç²å– store - ä¿®æ­£ç‰ˆ\n",
        "    store = None\n",
        "\n",
        "    # æ–¹æ³•1: ç›´æ¥å¾ config å–å¾—\n",
        "    if hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "        print(\"Store ä¾†æº: config.store\")\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "        print(\"Store ä¾†æº: config['store']\")\n",
        "\n",
        "    # æ–¹æ³•2: å¾ configurable ä¸­å–å¾—\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "        print(\"Store ä¾†æº: config['configurable']['store']\")\n",
        "\n",
        "    # æ–¹æ³•3: ä½¿ç”¨å…¨åŸŸè®Šæ•¸\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "        print(\"Store ä¾†æº: global_store\")\n",
        "\n",
        "    print(f\"Store ç‹€æ…‹: {store is not None}\")\n",
        "\n",
        "    # å¾ store å–å¾—ä½¿ç”¨è€…çš„é•·æœŸè¨˜æ†¶ - ä¿®æ­£ç‰ˆ\n",
        "    user_memory = None\n",
        "    if store:\n",
        "        try:\n",
        "            namespace = (\"user_profiles\", user_id)\n",
        "            print(f\"å˜—è©¦è®€å– namespace: {namespace}\")\n",
        "\n",
        "            # ä¿®æ­£ï¼šä½¿ç”¨ get æ–¹æ³•è€Œä¸æ˜¯ search\n",
        "            memory_item = store.get(namespace, \"profile\")\n",
        "            if memory_item:\n",
        "                user_memory = memory_item.value\n",
        "                print(f\"âœ… æ‰¾åˆ°ä½¿ç”¨è€…è¨˜æ†¶: {user_memory[:100]}...\")\n",
        "            else:\n",
        "                print(\"âŒ æ²’æœ‰æ‰¾åˆ°ä½¿ç”¨è€…è¨˜æ†¶\")\n",
        "\n",
        "            # é¡å¤–èª¿è©¦ï¼šä¹Ÿè©¦è©¦ search æ–¹æ³•\n",
        "            try:\n",
        "                memories = list(store.search(namespace))\n",
        "                print(f\"Search çµæœæ•¸é‡: {len(memories)}\")\n",
        "                for i, mem in enumerate(memories):\n",
        "                    print(f\"  Memory {i}: {str(mem)[:100]}...\")\n",
        "            except Exception as search_e:\n",
        "                print(f\"Search æ–¹æ³•éŒ¯èª¤: {search_e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"è®€å–é•·æœŸè¨˜æ†¶éŒ¯èª¤: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"âš ï¸  ç„¡æ³•ç²å– store\")\n",
        "\n",
        "    # æº–å‚™ç³»çµ±æç¤ºè©\n",
        "    system_prompt = \"ä½ æ˜¯ä¸€å€‹å‹å–„çš„èŠå¤©æ©Ÿå™¨äººåŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\"\n",
        "\n",
        "    if user_memory:\n",
        "        system_prompt += f\"\\n\\né—œæ–¼é€™å€‹ä½¿ç”¨è€…çš„è³‡è¨Šï¼š{user_memory}\\nè«‹æ ¹æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›æ‡‰ã€‚\"\n",
        "        print(\"âœ… å·²åŠ å…¥å€‹äººåŒ–è³‡è¨Šåˆ°ç³»çµ±æç¤ºä¸­\")\n",
        "    else:\n",
        "        print(\"âŒ æ²’æœ‰å€‹äººåŒ–è³‡è¨Šå¯ç”¨\")\n",
        "\n",
        "    # æ§‹å»ºå®Œæ•´çš„è¨Šæ¯åˆ—è¡¨\n",
        "    full_messages = [SystemMessage(content=system_prompt)]\n",
        "\n",
        "    # ä¿®å‰ªæ­·å²è¨Šæ¯\n",
        "    try:\n",
        "        trimmed_messages = trim_messages(\n",
        "            messages,\n",
        "            max_tokens=1500,\n",
        "            strategy=\"last\",\n",
        "            token_counter=count_tokens_approximately,\n",
        "            include_system=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        trimmed_messages = messages[-8:]\n",
        "\n",
        "    full_messages.extend(trimmed_messages)\n",
        "\n",
        "    response = llm.invoke(full_messages)\n",
        "    print(\"=== Chatbot Debug End ===\\n\")\n",
        "\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "f2PFRidLSusx"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_memory(state: State, *, config):\n",
        "    \"\"\"å°‡å°è©±è³‡è¨Šæ•´ç†ä¸¦å¯«å…¥é•·æœŸè¨˜æ†¶\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    print(f\"=== Write Memory Debug ===\")\n",
        "    print(f\"User ID: {user_id}\")\n",
        "\n",
        "    # å˜—è©¦ç²å– store - èˆ‡è®€å–é‚è¼¯ä¿æŒä¸€è‡´\n",
        "    store = None\n",
        "    if hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "\n",
        "    print(f\"Store ç‹€æ…‹: {store is not None}\")\n",
        "\n",
        "    if not store:\n",
        "        print(\"âš ï¸  ç„¡æ³•ç²å– storeï¼Œè·³éè¨˜æ†¶å¯«å…¥\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        # å–å¾—æœ€è¿‘çš„å°è©±å…§å®¹\n",
        "        recent_messages = messages[-4:] if len(messages) >= 4 else messages\n",
        "\n",
        "        # æ§‹å»ºç”¨æ–¼è¨˜æ†¶æ•´ç†çš„æç¤ºè©\n",
        "        memory_prompt = \"\"\"\n",
        "è«‹æ ¹æ“šä»¥ä¸‹å°è©±å…§å®¹ï¼Œæ•´ç†å‡ºä½¿ç”¨è€…çš„é‡è¦è³‡è¨Šã€‚\n",
        "è«‹æå–ä¸¦æ•´ç†ï¼š\n",
        "- ä½¿ç”¨è€…çš„å§“åæˆ–ç¨±å‘¼\n",
        "- åå¥½çš„èªè¨€\n",
        "- èˆˆè¶£å’Œæ„›å¥½\n",
        "- å€‹äººç‰¹å¾µæˆ–èƒŒæ™¯\n",
        "- å…¶ä»–é‡è¦çš„å€‹äººè³‡è¨Š\n",
        "\n",
        "è«‹ä»¥ç°¡æ½”çš„æ–¹å¼æè¿°é€™å€‹ä½¿ç”¨è€…ï¼Œå¦‚æœæ²’æœ‰æ–°çš„é‡è¦è³‡è¨Šï¼Œè«‹å›ç­”\"ç„¡æ–°å¢è³‡è¨Š\"ã€‚\n",
        "\n",
        "å°è©±å…§å®¹ï¼š\n",
        "\"\"\"\n",
        "\n",
        "        # æ·»åŠ å°è©±å…§å®¹\n",
        "        for msg in recent_messages:\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                memory_prompt += f\"ä½¿ç”¨è€…ï¼š{msg.content}\\n\"\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                memory_prompt += f\"åŠ©æ‰‹ï¼š{msg.content}\\n\"\n",
        "\n",
        "        # ä½¿ç”¨ LLM æ•´ç†è¨˜æ†¶\n",
        "        memory_response = llm.invoke([SystemMessage(content=memory_prompt)])\n",
        "        new_memory = memory_response.content\n",
        "\n",
        "        print(f\"åˆ†æå¾—åˆ°çš„æ–°è¨˜æ†¶: {new_memory}\")\n",
        "\n",
        "        # å¦‚æœæœ‰æ–°çš„è¨˜æ†¶è³‡è¨Šï¼Œå‰‡æ›´æ–° store\n",
        "        if new_memory and \"ç„¡æ–°å¢è³‡è¨Š\" not in new_memory:\n",
        "            namespace = (\"user_profiles\", user_id)\n",
        "\n",
        "            # å˜—è©¦ç²å–ç¾æœ‰è¨˜æ†¶ - ä¿®æ­£ç‰ˆ\n",
        "            existing_memory = \"\"\n",
        "            try:\n",
        "                memory_item = store.get(namespace, \"profile\")\n",
        "                if memory_item:\n",
        "                    existing_memory = memory_item.value\n",
        "                    print(f\"ç¾æœ‰è¨˜æ†¶: {existing_memory}\")\n",
        "            except Exception as e:\n",
        "                print(f\"è®€å–ç¾æœ‰è¨˜æ†¶éŒ¯èª¤: {e}\")\n",
        "\n",
        "            # åˆä½µæ–°èˆŠè¨˜æ†¶\n",
        "            if existing_memory:\n",
        "                combined_memory = f\"{existing_memory}\\n\\næ–°å¢è³‡è¨Šï¼š{new_memory}\"\n",
        "            else:\n",
        "                combined_memory = new_memory\n",
        "\n",
        "            # å„²å­˜åˆ° store\n",
        "            store.put(namespace, \"profile\", combined_memory)\n",
        "            print(f\"âœ… å·²ç‚ºä½¿ç”¨è€… {user_id} å„²å­˜è¨˜æ†¶\")\n",
        "\n",
        "            # ç«‹å³é©—è­‰å¯«å…¥\n",
        "            try:\n",
        "                verify_item = store.get(namespace, \"profile\")\n",
        "                if verify_item:\n",
        "                    print(f\"âœ… é©—è­‰æˆåŠŸï¼Œè¨˜æ†¶å·²å¯«å…¥: {verify_item.value[:50]}...\")\n",
        "                else:\n",
        "                    print(\"âŒ é©—è­‰å¤±æ•—ï¼Œè¨˜æ†¶å¯èƒ½æ²’æœ‰æ­£ç¢ºå¯«å…¥\")\n",
        "            except Exception as verify_e:\n",
        "                print(f\"é©—è­‰éŒ¯èª¤: {verify_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"è¨˜æ†¶å¯«å…¥éŒ¯èª¤: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"=== Write Memory Debug End ===\\n\")\n",
        "    return {}"
      ],
      "metadata": {
        "id": "BTMKnROpVlaT"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å…¨åŸŸ store è®Šæ•¸\n",
        "global_store = None"
      ],
      "metadata": {
        "id": "thbC4jt0bI_P"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_memory_graph():\n",
        "    \"\"\"å»ºç«‹åŒ…å«é•·æœŸè¨˜æ†¶çš„ graph - ä¿®æ­£ç‰ˆ\"\"\"\n",
        "    global global_store\n",
        "\n",
        "    builder = StateGraph(State)\n",
        "    builder.add_node(\"chatbot\", chatbot_with_memory)\n",
        "    builder.add_node(\"write_memory\", write_memory)\n",
        "    builder.add_edge(START, \"chatbot\")\n",
        "    builder.add_edge(\"chatbot\", \"write_memory\")\n",
        "    builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "    # ç·¨è­¯ graph\n",
        "    memory = MemorySaver()\n",
        "    store = InMemoryStore()\n",
        "    global_store = store  # ä¿å­˜å…¨åŸŸå¼•ç”¨\n",
        "\n",
        "    graph = builder.compile(checkpointer=memory, store=store)\n",
        "\n",
        "    print(f\"Graph å»ºç«‹å®Œæˆï¼ŒStore: {store is not None}\")\n",
        "    print(f\"Store ID: {id(store)}\")\n",
        "    return graph, store  # åŒæ™‚å›å‚³ store"
      ],
      "metadata": {
        "id": "o2sK1m4AVpb6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "JblBU3tecQ9-",
        "outputId": "1ba09e81-4d7b-4eb6-994f-4b71e6834ba2"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7e09e7f3ac50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ¸¬è©¦å’Œä½¿ç”¨å‡½æ•¸"
      ],
      "metadata": {
        "id": "JFqg436etzLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(graph, user_input: str, config: dict):\n",
        "    \"\"\"ä¸²æµè¼¸å‡º graph æ›´æ–°\"\"\"\n",
        "    try:\n",
        "        for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "            if \"chatbot\" in event:\n",
        "                for value in event.values():\n",
        "                    print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "    except Exception as e:\n",
        "        print(f\"ä¸²æµè™•ç†éŒ¯èª¤: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "D4VmdBPNVsUx"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_basic_chatbot():\n",
        "    \"\"\"æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äºº\"\"\"\n",
        "    print(\"=== æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ˆå«çŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰ ===\")\n",
        "    graph = create_basic_graph()\n",
        "    config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "\n",
        "    print(\"é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "o6osupNZV1Cc"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_memory_chatbot():\n",
        "    \"\"\"æ¸¬è©¦åŒ…å«é•·æœŸè¨˜æ†¶çš„èŠå¤©æ©Ÿå™¨äºº - ä¿®æ­£ç‰ˆ\"\"\"\n",
        "    print(\"=== æ¸¬è©¦é•·æœŸè¨˜æ†¶èŠå¤©æ©Ÿå™¨äºº ===\")\n",
        "    graph, store = create_memory_graph()  # æ¥æ”¶ store\n",
        "\n",
        "    print(f\"Main Store ID: {id(store)}\")\n",
        "    print(f\"Global Store ID: {id(global_store)}\")\n",
        "\n",
        "    # ä½¿ç”¨è€…Açš„ç¬¬ä¸€æ¬¡å°è©±\n",
        "    print(\"\\n--- ä½¿ç”¨è€…Açš„ç¬¬ä¸€æ¬¡å°è©± ---\")\n",
        "    # ä¿®æ­£ï¼šç›´æ¥å°‡ store åŠ å…¥ config\n",
        "    config1 = {\n",
        "        \"configurable\": {\n",
        "            \"thread_id\": \"conversation_1\",\n",
        "            \"user_id\": \"user_a\",\n",
        "            \"store\": store  # ç›´æ¥å‚³é store\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User A: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"çµæŸç¬¬ä¸€æ¬¡å°è©±!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config1)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break\n",
        "\n",
        "    # ä½¿ç”¨è€…Açš„ç¬¬äºŒæ¬¡å°è©±ï¼ˆæ–°çš„ threadï¼Œä½†åŒä¸€å€‹ userï¼‰\n",
        "    print(\"\\n--- ä½¿ç”¨è€…Açš„ç¬¬äºŒæ¬¡å°è©± ---\")\n",
        "    # ä¿®æ­£ï¼šå‰µå»ºæ–°çš„ configï¼Œä½†ä½¿ç”¨ç›¸åŒçš„ store\n",
        "    config2 = {\n",
        "        \"configurable\": {\n",
        "            \"thread_id\": \"conversation_2\",\n",
        "            \"user_id\": \"user_a\",\n",
        "            \"store\": store  # ä½¿ç”¨ç›¸åŒçš„ store å¯¦ä¾‹\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"é–‹å§‹ç¬¬äºŒæ¬¡å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User A: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"çµæŸç¬¬äºŒæ¬¡å°è©±!\")\n",
        "                break\n",
        "            stream_graph_updates(graph, user_input, config2)\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤: {e}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "I0Gt0JaDV3Dz"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€²éšç‰ˆåŠŸèƒ½"
      ],
      "metadata": {
        "id": "EEaBC4VlV5NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserProfile(BaseModel):\n",
        "    \"\"\"çµæ§‹åŒ–çš„ä½¿ç”¨è€…æª”æ¡ˆ\"\"\"\n",
        "    first_name: Optional[str] = None\n",
        "    last_name: Optional[str] = None\n",
        "    preferred_lang: List[str] = []\n",
        "    interests: List[str] = []\n",
        "    background: Optional[str] = None\n",
        "\n",
        "def advanced_write_memory(state: State, *, config):\n",
        "    \"\"\"é€²éšç‰ˆè¨˜æ†¶å¯«å…¥ï¼Œä½¿ç”¨çµæ§‹åŒ–è¼¸å‡º\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # ç²å– store çš„é‚è¼¯èˆ‡åŸºæœ¬ç‰ˆä¿æŒä¸€è‡´\n",
        "    store = None\n",
        "    if hasattr(config, 'store'):\n",
        "        store = config.store\n",
        "    elif 'store' in config:\n",
        "        store = config['store']\n",
        "    elif 'store' in config.get('configurable', {}):\n",
        "        store = config['configurable']['store']\n",
        "    else:\n",
        "        global global_store\n",
        "        store = global_store\n",
        "\n",
        "    if not store:\n",
        "        return {}\n",
        "\n",
        "    recent_messages = messages[-4:] if len(messages) >= 4 else messages\n",
        "\n",
        "    conversation_text = \"\"\n",
        "    for msg in recent_messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            conversation_text += f\"ä½¿ç”¨è€…ï¼š{msg.content}\\n\"\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            conversation_text += f\"åŠ©æ‰‹ï¼š{msg.content}\\n\"\n",
        "\n",
        "    structured_llm = llm.with_structured_output(UserProfile)\n",
        "\n",
        "    try:\n",
        "        profile_prompt = f\"\"\"\n",
        "        æ ¹æ“šä»¥ä¸‹å°è©±ï¼Œæå–ä½¿ç”¨è€…çš„è³‡è¨Šï¼š\n",
        "        {conversation_text}\n",
        "\n",
        "        è«‹æå–ï¼š\n",
        "        - first_name: ä½¿ç”¨è€…çš„åå­—\n",
        "        - last_name: ä½¿ç”¨è€…çš„å§“æ°\n",
        "        - preferred_lang: åå¥½èªè¨€åˆ—è¡¨ï¼ˆå¦‚ [\"zh-tw\", \"en\"]ï¼‰\n",
        "        - interests: èˆˆè¶£æ„›å¥½åˆ—è¡¨\n",
        "        - background: èƒŒæ™¯è³‡è¨Š\n",
        "\n",
        "        å¦‚æœæŸäº›è³‡è¨Šä¸æ˜ç¢ºï¼Œè«‹ç•™ç©ºã€‚\n",
        "        \"\"\"\n",
        "\n",
        "        new_profile = structured_llm.invoke([SystemMessage(content=profile_prompt)])\n",
        "        store.put((\"user_profiles\", user_id), \"structured_profile\", new_profile.dict())\n",
        "        print(f\"å·²å„²å­˜çµæ§‹åŒ–ä½¿ç”¨è€…æª”æ¡ˆ: {new_profile}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"çµæ§‹åŒ–è¨˜æ†¶å¯«å…¥éŒ¯èª¤: {e}\")\n",
        "\n",
        "    return {}"
      ],
      "metadata": {
        "id": "mrddF3FjV5kT"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¸¬è©¦"
      ],
      "metadata": {
        "id": "JcNpEgqoWDM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"èŠå¤©æ©Ÿå™¨äººè¨˜æ†¶å„ªåŒ–ä½œæ¥­\")\n",
        "print(\"1. æ¸¬è©¦åŸºæœ¬ç‰ˆï¼ˆçŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰\")\n",
        "print(\"2. æ¸¬è©¦é•·æœŸè¨˜æ†¶ç‰ˆ\")\n",
        "print(\"3. é€€å‡º\")\n",
        "\n",
        "while True:\n",
        "    choice = input(\"\\nè«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        test_basic_chatbot()\n",
        "    elif choice == \"2\":\n",
        "        test_memory_chatbot()3\n",
        "    elif choice == \"3\":\n",
        "        print(\"å†è¦‹ï¼\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡è©¦\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLp-M-u6WDAW",
        "outputId": "2539a5c8-2fa4-4705-9d97-29f671f7163d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "èŠå¤©æ©Ÿå™¨äººè¨˜æ†¶å„ªåŒ–ä½œæ¥­\n",
            "1. æ¸¬è©¦åŸºæœ¬ç‰ˆï¼ˆçŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰\n",
            "2. æ¸¬è©¦é•·æœŸè¨˜æ†¶ç‰ˆ\n",
            "3. é€€å‡º\n",
            "\n",
            "è«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): 1\n",
            "=== æ¸¬è©¦åŸºæœ¬ç‰ˆèŠå¤©æ©Ÿå™¨äººï¼ˆå«çŸ­æœŸè¨˜æ†¶å„ªåŒ–ï¼‰ ===\n",
            "é–‹å§‹å°è©± (è¼¸å…¥ 'quit', 'exit', 'q' åœæ­¢å°è©±)\n",
            "User: ä½ å¥½\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-108-3fe4c8b9022b>\", line 4, in stream_graph_updates\n",
            "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\", line 2436, in stream\n",
            "    for _ in runner.tick(\n",
            "  File \"<ipython-input-54-b7b75ba024bd>\", line 24, in chatbot_basic\n",
            "    response = llm.invoke(trimmed_messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 371, in invoke\n",
            "    \"ChatGeneration\",\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 956, in generate_prompt\n",
            "    prompt_messages = [p.to_messages() for p in prompts]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 775, in generate\n",
            "    results.append(\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1021, in _generate_with_cache\n",
            "    elif inspect.signature(self._generate).parameters.get(\"run_manager\"):\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 957, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1034, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1748995200000'}, 'provider_name': None}}, 'user_id': 'user_2w9gBJLw3SzVow6vIHt7E17gtjY'}\n",
            "During task with name 'chatbot' and id '1c0b3bef-64d8-078d-84ef-5d0002e209b7'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸²æµè™•ç†éŒ¯èª¤: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1748995200000'}, 'provider_name': None}}, 'user_id': 'user_2w9gBJLw3SzVow6vIHt7E17gtjY'}\n",
            "User: q\n",
            "Goodbye!\n",
            "\n",
            "è«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): \n",
            "ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡è©¦\n",
            "\n",
            "è«‹é¸æ“‡æ¸¬è©¦é …ç›® (1-3): 3\n",
            "å†è¦‹ï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDlYE2obiE1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}